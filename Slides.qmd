---
title: "Detecting Anomalous Cybersecurity Events Using LSTM Networks"
subtitle: "A Case Study on the BETH Dataset"
author: "Lionel & Killian (Advisor: Dr. Cohen)"
date: today
format: revealjs
course: Capstone Projects in Data Science
bibliography: references.bib
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

- Modern infrastructures face **sophisticated, process-level intrusions** that mimic benign behavior.
- **Static rule-based detection fails** to capture temporal dependencies.
- We investigate **LSTM-based anomaly detection** for process logs.
- **Key idea**: model normal system activity and detect deviations as anomalies.

---

### LSTM Motivation 

- **LSTM networks** (Hochreiter et al. 1997):
  - Solve vanishing gradient problem with gated memory cells.
  - Capable of learning **long-range temporal dependencies**.

- Applications:
  - **Speech recognition** (Graves et al. 2013).
  - **Language modeling** (Sundermeyer et al. 2012).
  - **Clinical anomaly detection** (Lipton et al. 2016).

- Why relevant ?
  - **Cybersecurity logs are sequential** — order and timing matter.

---

## Cybersecurity Context 

- **Kim et al. 2016**: LSTM classifiers for intrusion detection outperform static baselines.


- **Malhotra et al. 2016**: LSTM encoder-decoder reconstructs normal sequences, anomalies detected by high reconstruction error.


- **Yin et al. 2017**: RNNs generalize better on raw network flows.


- **Cinque et al. 2022**: Micro2vec + LSTM captures log anomalies in microservices.
---

## Data Challenge 

- Old datasets (KDD’99, NSL-KDD, ISCX 2012) are **synthetic and outdated**.


- **BETH dataset (Highnam et al. 2021)**[@inproceedings]:
  - 8M+ process-level events (eBPF-instrumented honeypots).
  - Rich features: timestamps, syscalls, labels (benign vs. malicious).
  - Realistic **temporal and adversarial patterns** for modern systems.
---

## Our Approach 

- **Semi-supervised LSTM Autoencoder**:
  - Train only on benign sequences (normality modeling).
  - Use **reconstruction error** to flag anomalies.

- **Goal**: detect process-level attacks in test logs.
---

## Methods
```{python}
## Setup and Data Loading

#| echo: true
#| warning: false
#| message: false

import gdown
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from load_datasets import load_all_datasets  # Make sure this module is available
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix, roc_auc_score, roc_curve
from tensorflow import keras
from keras import layers, callbacks, Model
import keras_tuner as kt
from sklearn.model_selection import TimeSeriesSplit
import os
os.environ["KERAS_BACKEND"] = "tensorflow"

# Load datasets
try:
    datasets = load_all_datasets()
    train_df = datasets["train"]
    val_df = datasets["validation"]
    test_df = datasets["test"]
    
    # Convert timestamps
    train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], unit='s', errors='coerce')
    val_df['timestamp'] = pd.to_datetime(val_df['timestamp'], unit='s', errors='coerce')
    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], unit='s', errors='coerce')
    
except Exception as e:
    print(f"Failed to load datasets: {str(e)}")
    raise
```
### Understanding LSTM Networks

- Recurrent Neural Network (RNN) with memory
- Keeps track of long sequences
- Ideal for logs, time series, and system calls


## LSTM Fundamentals

::: columns
::: {.column width="50%"}
<span style="font-size:10%">

- Forget Gate:  
  $f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$  

- Input Gate and Candidate Update:  
  $i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$  
  $\tilde{C}_t = \tanh(W_c [h_{t-1}, x_t] + b_c)$  

- Cell State Update:  
  $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$  

- Output Gate and Hidden State:  
  $o_t = \sigma(W_o [h_{t-1}, x_t] + b_o), \;\; h_t = o_t * \tanh(C_t)$  

</span>
:::

::: {.column width="50%"}
<div style="margin-left: 100px;">
  <img src="lstm_cell.png" style="width:100%;" />
  <p style="font-size: 80%; text-align: center; margin-top: 10px;">
    Figure 1: A module of LSTM network <span style="font-size: 70%;">[@trinh2021detecting]</span>
  </p>
</div>
:::
:::

---

##  LSTM Autoencoder for Anomaly Detection

![Figure 2: LSTM Autoencoder for Anomaly Detection [@trinh2021detecting]](11.png){fig-align="center" width=900px}

<span style="font-size:50%">Anomaly Detection from Reconstruction Error</span>

<div style="font-size:50%">
$$
L = \frac{1}{T} \sum_{t=1}^T \| x_t - \hat{x}_t \|^2
$$
</div>

---

## The BETH Dataset
- **Dataset size**: 8,004,918 system events.
- **Training subset**: ~763,145 benign events (60/20/20 split).

<div style="max-height: 300px; overflow-y: auto; font-size: 80%;">

| **Feature**         | **Description**                                             |
|---------------------|-------------------------------------------------------------|
| `timestamp`         | Date and time When the event occurred (float)               |
| `processId`         | ID of the process generating the event                      |
| `threadId`          | ID of the thread performing the operation                   |
| `parentProcessId`   | ID of the parent process                                    |
| `userId`            | User running the process/event                              |
| `mountNamespace`    | Kernel namespace for filesystem isolation                   |
| `processName`       | Name of the executable or program                           |
| `hostName`          | Name or IP of the machine                                   |
| `eventId`           | Numeric identifier for the event                            |
| `eventName`         | Name/type of system call/event                              |
| `stackAddresses`    | List of memory addresses (call stack)                       |
| `argsNum`           | Number of arguments for the event                           |
| `returnValue`       | Return value of the system call/event                       |
| `args`              | List of arguments (name, type, value)                       |
| `sus`               | 1 if flagged suspicious, 0 otherwise                        |
| `evil`              | 1 if event is malicious, 0 otherwise                        |

</div>

---

## Data Preprocessing and Sequence Modeling
 
### Preparing the Data

- Feature Selection  

- Encoding:  
  One-hot encoding for categorical features  
  
- Sequence Generation:  sliding window and Reshaped logs into 3D tensors

- Sequence Labeling:  
  Normal: Only benign events  
  Anomalous: At least one suspicious (Sus) or malicious (Evil) event  

---

## LSTM Training & Architecture 
  
    
Trained on normal sequences only  

Symmetric autoencoder: stacked LSTM + bottleneck  

Optimized via Adam, minimizing reconstruction loss  

Generalization: early stopping, dropout, 5-fold CV  

Hyperparameters tuned based on prior work  [@nguyen2021forecasting; @malhotra2016lstm].
---

## Threshold Selection and Evaluation Metrics

Threshold selection via:  

  - 95th percentile of training errors  
  - F1-score optimization (validation set)  
  - Joint minimization of FP/FN rates  

Evaluation metrics:  

  - Precision, Recall, F1-score  
  - ROC-AUC, Confusion Matrix  
  - Error distribution plots  
---

## Data Exploration and Visualization

####  Event Frequency Distribution.

```{python}
# ---- FIGURE 1: Most common event names in training data ----
plt.figure(figsize=(10, 5))
top_events = train_df['eventName'].value_counts().nlargest(10)
sns.barplot(x=top_events.values, y=top_events.index, palette='Blues_r')
plt.title("Top 10 Event Names in Training Data")
plt.xlabel("Frequency")
plt.ylabel("Event Name")
plt.tight_layout()
plt.show()
```

---

##  Process Behavior

```{python}
# ---- FIGURE 2: Most common process names in training data ----
plt.figure(figsize=(10, 5))
top_events = train_df['processName'].value_counts().nlargest(10)
sns.barplot(x=top_events.values, y=top_events.index, palette='viridis')
plt.title("Top 10 process Names in Training Data")
plt.xlabel("Frequency")
plt.ylabel("process_Name")
plt.tight_layout()
plt.show()
```
---

## Temporal Skew in Data Collection

```{python}
# ---- FIGURE 2: Event frequency by hour ----
train_df['hour'] = train_df['timestamp'].dt.hour
plt.figure(figsize=(10, 5))
sns.countplot(data=train_df, x='hour', palette='viridis')
plt.title("System Events Distribution by Hour (Training Data)")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Events")
plt.tight_layout()
plt.show()
```

---

## Label Imbalance and Anomaly Prevalence  
```{python}
# ---- FIGURE 3: Count of evil vs. benign events in test set ----
plt.figure(figsize=(6, 4))
sns.countplot(data=test_df, x='evil', palette='coolwarm')
plt.title("Malicious vs. Benign Events (Test Set)")
plt.xlabel("Evil Label")
plt.ylabel("Count")
plt.xticks([0, 1], ['Benign (0)', 'Malicious (1)'])
plt.tight_layout()
plt.show()
```

---

##  Correlation

```{python}
def dataset_to_corr_heatmap(dataFrame, title, ax=None):
    corr = dataFrame.corr(numeric_only=True)
    sns.heatmap(corr, annot=True, cmap="YlGnBu", ax=ax, annot_kws={"size": 8})
    if ax is None:
        plt.title(f'Correlation: {title}')
    else:
        ax.set_title(f'Correlation: {title}', pad=20)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))


dataset_to_corr_heatmap(train_df, 'Train Dataset', ax1)

dataset_to_corr_heatmap(test_df, 'Test Dataset', ax2)

plt.tight_layout(pad=3.0)
plt.show()
```
---

##  Correlation
```{python}
plt.figure()
dataset_to_corr_heatmap(test_df, 'validation')
plt.show()
```

---


##  Boxplot of entropy values

```{python}
from scipy import stats
import matplotlib.pyplot as plt

datasets = [train_df, test_df, val_df]
dataset_labels = ['train', 'test', 'validate']

# --- Entropy Calculation ---
entropy_values = []
for dataset in datasets:
    dataset_entropy_values = []
    for col in dataset.columns:
        if col != 'timestamp':
            counts = dataset[col].value_counts()
            col_entropy = stats.entropy(counts)
            dataset_entropy_values.append(col_entropy)
    entropy_values.append(dataset_entropy_values)

# --- Variation Calculation ---
variation_values = []
for dataset in datasets:
    dataset_variation_values = []
    for col in dataset.columns:
        if col != 'timestamp':
            counts = dataset[col].value_counts()
            col_variation = stats.variation(counts)
            dataset_variation_values.append(col_variation)
    variation_values.append(dataset_variation_values)

# --- Plotting both boxplots side by side ---
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(14, 6))

# Entropy boxplot
ax1.boxplot(entropy_values)
ax1.set_title('Boxplot of Entropy Values')
ax1.set_ylabel("Entropy")
ax1.set_xticks([1, 2, 3])
ax1.set_xticklabels(dataset_labels)

# Variation boxplot
ax2.boxplot(variation_values)
ax2.set_title('Boxplot of Variation Values')
ax2.set_ylabel("Variation")
ax2.set_xticks([1, 2, 3])
ax2.set_xticklabels(dataset_labels)

plt.tight_layout()
plt.show()
```
---


## Model Architecture {.smaller}

```{python}
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Assign ground truth: 0=benign (evil=0), 1=malicious (evil=1)
for df in [train_df, val_df, test_df]:
    df['ground_truth'] = df['evil'].apply(lambda x: 1 if x == 1 else 0)

# Select features
features = [
    'processId', 'parentProcessId', 'userId', 'mountNamespace',
    'processName', 'hostName', 'eventId', 'argsNum', 'returnValue'
]

# Label encode categorical columns
for col in ['processName', 'hostName']:
    le = LabelEncoder()
    all_vals = pd.concat([train_df[col], val_df[col], test_df[col]])
    le.fit(all_vals)
    train_df[col] = le.transform(train_df[col])
    val_df[col] = le.transform(val_df[col])
    test_df[col] = le.transform(test_df[col])

# Scale features
scaler = StandardScaler()
train_df[features] = scaler.fit_transform(train_df[features])
val_df[features] = scaler.transform(val_df[features])
test_df[features] = scaler.transform(test_df[features])
```

```{python}
def create_sequences(df, features, look_back=8):
    data_array = df[features].values
    labels = df['ground_truth'].values
    X, y = [], []
    for i in range(len(df) - look_back + 1):
        X.append(data_array[i:i+look_back])
        y.append(labels[i + look_back - 1])
    return np.array(X), np.array(y)


look_back = 8

X_train, _ = create_sequences(train_df, features, look_back)
X_val, _ = create_sequences(val_df, features, look_back)
X_test, y_test = create_sequences(test_df, features, look_back)

print("X_train shape:", X_train.shape)
print("X_val shape:", X_val.shape)
print("X_test shape:", X_test.shape)
print("y_test distribution:", np.bincount(y_test))

```

```{python}
from tensorflow.keras import layers, Model
from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input
from tensorflow.keras import layers
import tensorflow as tf
from tensorflow import keras


def build_lstm_autoencoder(input_shape, latent_dim=64, dropout=0.1, learning_rate=1e-3):
    inputs = layers.Input(shape=input_shape)
    
    # Encoder
    x = layers.LSTM(128, activation='relu', return_sequences=True)(inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LSTM(latent_dim, activation='relu', return_sequences=False)(x)
    encoded = layers.Dropout(dropout)(x)
    
    # Repeat vector for decoder (Bottleneck)
    x = layers.RepeatVector(input_shape[0])(encoded)
    
    # Decoder
    x = layers.LSTM(latent_dim, activation='relu', return_sequences=True)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.LSTM(128, activation='relu', return_sequences=True)(x)
    x = layers.Dropout(dropout)(x)
    decoded = layers.TimeDistributed(layers.Dense(input_shape[1]))(x)

    model = Model(inputs, decoded)
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), 
                 loss='mae')
    return model

input_shape = X_train.shape[1:]
model = build_lstm_autoencoder(input_shape, latent_dim=64, dropout=0.1, learning_rate=1e-3)
model.summary()
```

##  Training and Validation Performance

- 5-fold cross-validation:

  - Avg. validation loss: **0.1516 ± 0.1443.**

- Stable convergence.

![Figure 7: Training / validation Loss result](6.png){width=800px}

## Anomaly Detection Threshold 

Distribution of reconstruction error separates normal vs. anomalous.

**Figure 6:** Reconstruction Error Distribution

![Figure 8: Histogram of test reconstruction errors (MAE)](7.png){fig-align="center" width=800px}   

##  Classification Metrics 

Precision: 99.3%, Recall: 99.5%, F1-score: 99.4%.

**Figure 7:** Precision/Recall Curve

![Figure 9: Precision Recall, F1 vs threshold plot](8.png){fig-align="center" width=600px}

## Confusion Matrix 


![Figure 10: Confusion matrix for final predictions](9.png){fig-align="center" width=600px}

## ROC Curve 

ROC AUC = 99.5%.

**Figure 9:** ROC Curve

![Figure 11: ROC Cuve](10.png){fig-align="center" width=600px} 
---
## Conclusion {.smaller}

*Key Results*  
 - LSTM autoencoder successfully learned normal process behavior from the BETH dataset.
 
 - Achieved 99% accuracy and F1-score of 0.95 in detecting malicious sequences.
 
 - High recall → Most attacks detected (low false negatives).
 
 - Low false positive rate (3.4%) → Suitable for real-world deployment.
 
*Implications*  
 - Detects novel/zero-day attacks without prior knowledge.
  
 - Fits real-time monitoring in dynamic/cloud environments.
 
 * Future work: add attention, ensemble methods, and retraining to improve adaptability.
---

## References
