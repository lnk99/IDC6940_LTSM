---
title: "Capstone project on LTSM"
subtitle: "This is a Report Template Quarto"
author: "Lionel & Killian (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: inline
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

## Introduction

With ever-growing complexity and interconnectedness in modern digital infrastructures, the attack surface has expanded appreciably, posing serious threats for organizations and individuals. Cybersecurity breaches, specifically advanced process-level intrusions aimed at crucial systems, have increased in quantity and complexity. Traditional rule-based detection tools and static feature engineering solutions are often ineffective for finding new threats exploiting temporal patterns or masquerading as benign activity. To meet this state, there is a need for the construction of resilient, dynamic, and data-driven models able to learn system activity in the real world to effectively identify anomalies.
In sequence modeling machine learning techniques, the Long Short-Term Memory (LSTM) networks have been important for the learning of long-range temporal relationships in sequential data such as speech (Graves et al., 2013), language (Sundermeyer et al., 2012), and biomedical signals (Lipton et al., 2016). Initially introduced by Hochreiter and Schmidhuber (1997), the LSTM networks enhance the traditional recurrent neural network (RNN) architecture through the addition of memory cells and gate mechanisms for the storage and selective forgetfulness of long-range time-series relationships. For these reasons, the use of LSTM is a natural choice in the representation of the behavior of the host in cybersecurity systems where the order of the execution of processes, the timing, and the context in which they are executed are of crucial semantic information.
Despite the promise of deep learning for the application of anomaly detection, most of the cybersecurity work relies on outdated or synthetic datasets such as the KDD Cup 1999 (Hettich & Bay, 1999), NSL-KDD (Tavallaee et al., 2009), and the ISCX IDS 2012 (Shiravi et al., 2012). Such datasets are lacking in the sort of diversity, realism, and granularity required to validate models within modern operating conditions. For instance, the BPF-extended tracking honeypot (BETH) dataset (Highnam et al., 2021) is a quantum leap in the field. It consists of well over eight million process-level events, meticulously labelled and gathered through extended Berkeley Packet Filter (eBPF) instrumentation on live cloud-hosted honeypots. It is important to note that the dataset consists of benign and adversarial host activity in natural operating conditions along with comprehensive temporal details and proper labels.
In our paper, we discuss the use of LSTM networks to characterize benign system activity and discern process-level anomalies through the usage of the BETH dataset. Unlike the common supervised learning scenario in which there are annotated examples of all attack modes, we embrace the semi-supervised method to detecting anomalies: the LSTM model is trained only on benign activity in order to reveal the hidden structure and temporal dynamics of typical host behavior. Under test, deviations of the acquired pattern are signaled as potential anomalies. Our contributions are the following:
We propose a sequence-based LSTM model trained on real-world benign process logs for anomaly detection. We benchmark the model on a modern, cloud-native dataset (BETH) containing labeled attack sequences. We evaluate the model using both classification metrics (e.g., precision, recall) and temporal anomaly scoring, comparing its performance to traditional baselines. This study contributes to the growing body of literature on robust machine learning for cybersecurity and demonstrates the applicability of temporal deep learning models in detecting nuanced and context-dependent threats in dynamic environments.


## Methods

The proposed methodology for this study is built upon a structured deep learning pipeline, beginning with dataset selection and preprocessing, followed by model construction and training, and concluding with anomaly detection and performance evaluation. Our study utilizes the BETH cybersecurity dataset, which comprises multivariate temporal system log entries from kernel-level processes. These logs include crucial features such as timestamps, syscall types, return values, arguments, and user context information, key indicators of system behavior. To prepare the data for modeling, raw logs are parsed and converted into structured formats. Categorical features are encoded using one-hot encoding, while continuous variables are scaled using min-max normalization. To retain temporal coherence, logs are segmented into fixed-length sequences using a sliding window approach. These sequences are formatted into 3D tensors with dimensions corresponding to batch size, time steps, and feature vector length, ensuring compatibility with LSTM input requirements and preserving event chronology.
At the core of the model architecture lies a Long Short-Term Memory (LSTM) autoencoder, designed to capture long-range dependencies in sequential log data. The encoder consists of stacked LSTM layers that compress each input sequence into a latent vector, while the decoder reconstructs the original input from this representation. The model is trained exclusively on normal sequences, under the assumption that anomalies, unseen during training will manifest as higher reconstruction errors. The training process minimizes mean squared error between input and reconstructed sequences. Post-training, reconstruction errors are computed on validation and test sets. A threshold is then determined based on the distribution of reconstruction errors in the validation set; sequences exceeding this threshold are labeled as anomalous. This unsupervised approach is particularly advantageous in cybersecurity contexts where anomalous patterns are sparse or ill-defined, allowing the system to generalize to previously unseen behaviors without the need for explicit labels.
For model evaluation, both quantitative and qualitative techniques are employed. Quantitative metrics include precision, recall, F1-score, and the area under the Receiver Operating Characteristic curve (ROC-AUC), computed using ground truth labels available in the BETH dataset. These metrics provide insight into the modelâ€™s classification accuracy and balance between false positives and false negatives. In addition, confusion matrices are analyzed to understand misclassification behavior, and reconstruction error distributions are visualized to assess the separation between normal and anomalous sequences. This comprehensive methodology integrates state-of-the-art sequence modeling, log-based feature engineering, and unsupervised anomaly detection, offering a scalable and interpretable framework for detecting cybersecurity threats in complex network environments


*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*


*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
