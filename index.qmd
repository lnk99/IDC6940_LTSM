---
title: "Detecting Anomalous Cybersecurity Events Using LSTM Autoencoders"
subtitle: "A Case Study on the BETH Dataset"
author: "Lionel & Killian (Advisor: Dr. Cohen)"
date: today
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
jupyter: python3
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: inline
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## I- Introduction

As modern digital infrastructures become increasingly interconnected and complex, attackers are relying more on process-level intrusions that carefully blend into legitimate activity. Such sophisticated threats often unfolding slowly over time or interleaving malicious commands with normal operations can evade static, signature-based defenses. Traditional systems, which are designed around pre-defined rules or pattern matching, fail to account for subtle changes in temporal context: an action that appears normal in isolation may signal an attack when it occurs in an unusual sequence. Detecting these threats requires models capable of understanding how events unfold over time.
This challenge has spurred research toward dynamic, data-driven sequence modeling methods. Among these, Long Short-Term Memory (LSTM) networks have emerged as particularly effective. Introduced by Hochreiter and Schmidhuber [@hochreiter1997long], LSTM networks use gated memory cells that retain information across long sequences, effectively overcoming the vanishing gradient problem in traditional recurrent neural networks. This architecture allows LSTMs to capture dependencies spanning dozens or even hundreds of events, making them well-suited for modeling the temporal structure of cybersecurity logs, where the sequence of system calls and process operations is often critical.

The versatility of LSTM networks has been demonstrated across diverse domains. In speech recognition, Graves, Mohamed, and Hinton [@graves2013speech] showed that deep LSTM architectures can learn the temporal structure of audio signals, significantly reducing error rates on benchmark tasks. In natural language processing, Sundermeyer et al. [@sundermeyer2012lstm] used LSTMs for language modeling, demonstrating that the model’s ability to retain long-range context leads to more accurate text generation compared to traditional n-gram methods. In the clinical domain, Lipton et al. [@lipton2016learning] applied LSTM models to patient vitals time series, revealing how memory-enabled models can detect subtle health anomalies as they develop. Across these applications, a core insight emerges: when the data is fundamentally sequential, LSTMs excel at modeling it.
This modeling power has translated into cybersecurity research as well. Kim et al. [@kim2016long] applied LSTM-based classifiers to intrusion detection, demonstrating that LSTMs can recognize abnormal sequences of system and network behavior with lower false-positive rates than static feature approaches. Malhotra et al. [@malhotra2016lstm] advanced this work by introducing an LSTM encoder decoder for anomaly detection. Instead of directly classifying events, the model reconstructs input sequences, identifying anomalies by elevated reconstruction errors an approach especially well suited to unlabeled or novel attacks, as it learns solely from normal behavior. Yin et al. [@yin2017deep] extended this concept using recurrent neural networks over raw flow data, achieving improved generalization in network intrusion detection. Building further, Cinque et al. [@cinque2022micro2vec] introduced Micro2vec, which embeds log message patterns as numeric vectors and leverages LSTM networks to detect anomalies in microservice environments, showing how sequence models can scale to complex, real-world systems.

Yet, progress in these models has often been constrained by outdated benchmarks. Seminal datasets such as KDD Cup 1999 [@hettich1999uci], NSL-KDD [@tavallaee2009detailed], and ISCX 2012 [@shiravi2012developing] provided early test beds for intrusion detection research, but are now hampered by synthetic traffic, unrealistic features, and obsolete system contexts limiting their generalizability. A model trained on these datasets may detect known patterns, but often fails in real-world scenarios where attackers use new commands, timing patterns, or process chains.

In response, Highnam et al. [@highnam2021beth] introduced the BETH dataset, marking a significant advance in realism and operational relevance. Instrumented with eBPF across live honeypots, BETH captures over eight million process-level events including system calls, arguments, return values, and timestamps. Importantly, each data instance cleanly separates benign system behavior from adversarial activity, providing labeled, temporally coherent sequences. This makes BETH an ideal platform for training sequence-based anomaly detection models in a semi-supervised setting.
Our approach builds directly on this foundation. We deploy an LSTM autoencoder trained exclusively on benign process sequences from BETH. The encoder compresses temporal sequences into fixed-length latent representations, while the decoder attempts to reconstruct them. When the model encounters novel malicious behavior, its reconstruction fails, producing high reconstruction error, this serves as our anomaly signal. This method aligns with recent advances in unsupervised anomaly detection [@gokstorp2024anomaly; @greff2017lstm], removing the need for prior knowledge of attack types and allowing rapid adaptation to emerging threats.

This work makes three key contributions. First, it demonstrates the effectiveness of LSTM-based sequence modeling for detecting anomalies in real-world, process-level logs. Second, it presents a rigorous evaluation on the BETH dataset, using classification metrics and reconstruction thresholds to quantify detection capabilities. Third, it bridges the gap between advanced deep learning techniques and practical cybersecurity needs, delivering an interpretable, flexible, and deployable detection framework grounded in realistic system behavior.


## II- Methods
This section presents the methodological framework for detecting anomalies in cybersecurity event logs using Long Short-Term Memory (LSTM) autoencoders. The approach is organized into six interconnected components and together, these steps form a comprehensive and scalable pipeline for identifying irregularities in large-scale log data

### 1. Long Short-Term Memory (LSTM) Networks

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) developed by Hochreiter and Schmidhuber (1997) [@hochreiter1997long] specifically to overcome the vanishing gradient problem found in traditional RNNs. In cybersecurity applications, the ability to model long-range temporal dependencies is crucial, as malicious activity often unfolds over extended sequences of system calls, network packets, or access attempts.

An LSTM cell maintains two key internal states at each time step: the cell state $C_t$ and the hidden state $h_t$. The cell state serves as a memory that runs across the sequence, allowing information to persist over time. Three gating mechanisms regulate this process:

- **Forget Gate**: Determines which parts of the previous memory to discard.  
  $$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$$
where $h_{t-1}$ is the output in state $t-1$, $W_f$ and $b_f$ is the weight matrices
and the bias of the forget gate.  

- **Input Gate and Candidate Update**: Controls how much new information is added to the memory.  
  $$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i), \;\; \tilde{C}_t = \tanh(W_c [h_{t-1}, x_t] + b_c)$$
- **Cell State Update**:  
  $$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$
where ($W_i$, $b_i$) and ($W_c$, $b_c$) are the weight matrices and the biases of
input gate and memory cell state, respectively

- **Output Gate and Hidden State**: Controls what the cell outputs.  
  $$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o), \;\; h_t = o_t * \tanh(C_t)$$
where $W_o$ and $b_o$ are the weight matrix and the bias of output gate, determines a part of the cell state being outputed.

![Figure 1: A module of LSTM network [@trinh2021detecting]](lstm_cell.png){fig-align="center" width=400px}

LSTM’s memory mechanisms make it especially effective for modeling system behavior over time, where a sequence of seemingly benign events may, when considered together, signal an attack. This makes LSTM a preferred choice for modeling logs in intrusion detection systems [@lipton2016learning; @greff2017lstm; @graves2013speech]. 

### 2. LSTM Autoencoder for Anomaly Detection

An LSTM autoencoder combines an encoder–decoder architecture with LSTM layers to learn latent representations of temporal data in an unsupervised manner. The encoder compresses input sequences into a fixed-length latent vector, while the decoder attempts to reconstruct the sequence from this vector. If the model is trained exclusively on normal data, it learns to reconstruct familiar sequences accurately. Anomalous sequences, which deviate from learned patterns, yield high reconstruction error a signal of abnormal behavior.

Formally, for an input sequence $x = (x_1, ..., x_T)$, the autoencoder is trained to minimize the reconstruction loss:

$$
L = \frac{1}{T} \sum_{t=1}^T \| x_t - \hat{x}_t \|^2
$$

where $\hat{x}_t$ is the output generated by the decoder at each time step.

During inference, this reconstruction error serves as an anomaly score. A sequence with error above a threshold $\theta$ is flagged as anomalous. This technique, called reconstruction-based anomaly detection, is effective when anomalous samples are rare or unknown, which is often the case in cybersecurity contexts.

![Figure 2: LSTM Autoencoder for Anomaly Detection [@trinh2021detecting]](11.png){fig-align="center" width=400px}
 
Past research has demonstrated the effectiveness of this approach in domains such as multi-sensor fault detection [@malhotra2016lstm], system log anomaly detection [@gokstorp2024anomaly], and network intrusion detection [@kim2016long; @cinque2022micro2vec]. Its unsupervised nature and ability to model complex time-dependent features make it a natural fit for the challenges of modern intrusion detection systems.

### 3. Anomaly Detection Using LSTM Autoencoders

Once trained on normal data, an LSTM autoencoder becomes a powerful tool for identifying anomalous behavior in sequential inputs such as system logs or network traces. The core idea is that the model becomes highly proficient at reconstructing sequences that follow the familiar temporal patterns it learned during training. However, when presented with sequences that deviate from these learned patterns such as those containing rare, unexpected, or malicious events, the model fails to reconstruct them accurately, producing a noticeably higher reconstruction error.

This reconstruction error serves as a quantifiable measure of anomaly. Specifically, the anomaly score for a given input sequence is computed as the **mean squared error (MSE)** between each input vector  $x_t$ and its reconstructed counterpart $x_t$:


$$\text{Reconstruction Error} = \frac{1}{T} \sum_{t=1}^{T} \|x_t - \hat{x}_t\|^2$$

Here,  $T$ is the total number of time steps in the sequence. A higher score indicates greater divergence from normal behavior, and thus, a higher likelihood that the sequence is anomalous.

To operationalize this detection process, a **threshold** is established. This threshold is usually selected by analyzing the distribution of reconstruction errors in the training or validation dataset, often by selecting a statistical percentile or optimizing detection metrics such as the F1-score. Any sequence with a reconstruction error exceeding the threshold is classified as **anomalous**, while sequences with lower errors are assumed to represent **normal activity**.

This approach offers several key benefits in the context of cybersecurity anomaly detection:

- **Unsupervised learning**: There is no need for labeled attack data during training, which is ideal since real-world attack examples are rare, imbalanced, or unknown at the time of deployment.
- **Temporal awareness**: LSTM cells allow the model to consider context across long event sequences—making it effective in identifying subtle but meaningful deviations in system behavior over time.
- **Detection of unknown attacks**: Since the model is not trained on known attack signatures, it generalizes better to novel or zero-day threats, simply by recognizing that "this sequence doesn't look like normal behavior."

The method has been successfully used in a variety of security-critical environments. For instance, Malhotra et al. [@malhotra2016lstm] used it to detect failures in multi-sensor environments, while Gökstorp et al. [@gokstorp2024anomaly] applied it to analyze security logs. These studies show that LSTM autoencoders can reliably distinguish benign from anomalous sequences, even in high-noise or high-volume contexts.

In this project, we leverage the LSTM autoencoder’s ability to detect such behavioral anomalies in the BETH dataset. By training solely on benign log sequences and evaluating reconstruction performance on new data, our model provides a flexible, adaptive, and scalable mechanism for real-time anomaly detection.

### 4. Justification for Approach and Dataset

Cybersecurity anomaly detection requires models that are flexible, context-aware, and capable of adapting to new or unseen attack patterns. Traditional rule-based or signature-based detection systems are limited by their reliance on predefined patterns, making them ineffective against zero-day attacks or novel behavior.

Unsupervised learning, particularly autoencoder-based methods, allows us to model what “normal” looks like and detect deviations from it, without requiring labeled attacks. This approach is grounded in the notion that abnormal behavior is, by definition, rare and structurally distinct from routine system activity [@yin2017deep].

The BETH dataset [@highnam2021beth] was selected to evaluate our approach. It includes over 8 million system log events collected from 23 honeypots designed to mimic real-world server environments. Each honeypot was exposed to the internet and captured one targeted attack (at most), alongside a large volume of benign activity. Events are labeled using two binary fields: Sus for suspicious activity and Evil for confirmed malicious events. the table below give a description of our all dataset. 

Table1: The description and type of each feature within the DNS logs  

| **Feature**         | **Description**                                             |
|---------------------|-------------------------------------------------------------|
| `timestamp`         | Date and time When the event occurred (float)               |
| `processId`         | ID of the process generating the event                      |
| `threadId`          | ID of the thread performing the operation                   |
| `parentProcessId`   | ID of the parent process                                    |
| `userId`            | User running the process/event                              |
| `mountNamespace`    | Kernel namespace for filesystem isolation                   |
| `processName`       | Name of the executable or program                           |
| `hostName`          | Name or IP of the machine                                   |
| `eventId`           | Numeric identifier for the event                            |
| `eventName`         | Name/type of system call/event                              |
| `stackAddresses`    | List of memory addresses (call stack)                       |
| `argsNum`           | Number of arguments for the event                           |
| `returnValue`       | Return value of the system call/event                       |
| `args`              | List of arguments (name, type, value)                       |
| `sus`               | 1 if flagged suspicious, 0 otherwise                        |
| `evil`              | 1 if event is malicious, 0 otherwise                        |


**The BETH dataset offers several key advantages:**  
- **Realism:** Data is collected in production-like environments with realistic attack vectors.  
- **Anomaly-focused labeling:** The Sus and Evil labels allow fine-grained evaluation of anomaly detection systems.  
- **Accessibility:** The dataset is publicly available, enabling reproducibility and peer comparison.  

Compared to older datasets like KDD’99 [@tavallaee2009detailed; @hettich1999uci] or NSL-KDD, BETH reflects modern system-level behavior and attack techniques, providing a more accurate benchmark for current anomaly detection research.
```{python}
!pip install keras-tuner
!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow keras-tuner
!pip install gdown pandas numpy matplotlib seaborn scikit-learn tensorflow keras-tuner
```

### 5. Data Preprocessing and Sequence Modeling

```{python}
## Setup and Data Loading

#| echo: true
#| warning: false
#| message: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from load_datasets import load_all_datasets  # Make sure this module is available
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix, roc_auc_score, roc_curve
from tensorflow import keras
from keras import layers, callbacks, Model
import keras_tuner as kt
from sklearn.model_selection import TimeSeriesSplit
import os
os.environ["KERAS_BACKEND"] = "tensorflow"

# Load datasets
try:
    datasets = load_all_datasets()
    train_df = datasets["training"]
    val_df = datasets["validation"]
    test_df = datasets["test"]
    
    # Convert timestamps
    train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], unit='s', errors='coerce')
    val_df['timestamp'] = pd.to_datetime(val_df['timestamp'], unit='s', errors='coerce')
    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], unit='s', errors='coerce')
    
except Exception as e:
    print(f"Failed to load datasets: {str(e)}")
    raise
```

To prepare the BETH dataset for LSTM modeling, we applied the following preprocessing steps:

- **Feature extraction:** Key fields such as Timestamp, EventID, SyscallType, ProcessId, ReturnValue, and ArgsNum were selected based on relevance to behavioral analysis.
- **Encoding:** Categorical fields were one-hot encoded to retain discrete information. Numerical fields were normalized to the [0, 1] range using min-max scaling.
- **Sequence generation:** A sliding window of fixed size (e.g., 25–50 events) was used to convert raw logs into overlapping sequences. Each sequence was reshaped into a 3D tensor of shape (Batch × Time × Features), preserving temporal order and structural coherence.

**Labeling for sequences was handled as follows:**
- Sequences with only benign events were labeled as normal.
- Sequences containing at least one Sus or Evil event were labeled as anomalous.

This approach mirrors real-world monitoring scenarios, where logs are processed in fixed-length windows to detect behavioral anomalies over time.

### 6. Model Training, Thresholding, and Evaluation

#### 6.1 Training and Architecture

The LSTM autoencoder is trained solely on normal sequences. The architecture includes two stacked LSTM layers in the encoder and decoder, with a fully connected layer at the bottleneck. The decoder mirrors the encoder in structure, and the model is trained to minimize reconstruction loss using the Adam optimizer.

To improve generalization:
- Early stopping is used based on validation loss.
- A dropout layer is added to mitigate overfitting.
- 5-fold cross-validation is used to assess the stability of the model across different subsets of the benign data.

Hyperparameters (e.g., learning rate, batch size, window size, latent vector size) were selected based on preliminary tuning and aligned with best practices from related work [@nguyen2021forecasting; @malhotra2016lstm].

#### 6.2 Threshold Selection

After training, we determine an anomaly threshold $\theta$ by analyzing reconstruction errors on a labeled validation set. Multiple strategies are considered:

- Using a fixed percentile of training error distribution (e.g., 95th percentile)
- Optimizing the F1-score on validation anomalies
- Minimizing false positive and false negative rates jointly

The selected threshold is applied to test sequences during evaluation.

#### 6.3 Evaluation Metrics

The model’s performance is evaluated using the following metrics:

- **Precision:** The proportion of predicted anomalies that are true anomalies.
- **Recall:** The proportion of actual anomalies that are correctly identified.
- **F1-score:** The harmonic mean of precision and recall.
- **ROC-AUC:** Measures the trade-off between true positive and false positive rates.
- **Confusion Matrix:** Provides a granular view of model classification results.
- **Error distribution plots:** Visualize separation between normal and anomalous samples.

These metrics provide a robust, multidimensional assessment of the model’s effectiveness in identifying anomalous behavior.

## III- Analysis and Results

### 1- Data Exploration and Visualization

To better understand the structure and composition of the BETH dataset, we begin by analyzing the process logs that serve as input to our LSTM-based anomaly detection model. The dataset comprises a total of 8,004,918 system events, among which a benchmark subset of approximately 763,145 records is reserved for training. The dataset is rich in temporal and categorical information, including process and thread identifiers, process names, system call arguments, return values, and two binary labels: sus (suspicious) and evil (malicious). The logs are chronologically ordered and contain a temporal feature (timestamp) suitable for sequential modeling. Each record represents an individual system event collected from one of 23 cloud-based honeypots. The data was split into training, validation, and test sets using a 60/20/20 ratio, where only the test set contains attack sequences.
A descriptive analysis of the features reveals the heterogeneous nature of the dataset. The eventName field, for example, exhibits a long-tailed distribution, with certain events (e.g., execve, read, openat) appearing frequently, while others are rare and potentially indicative of anomalous behavior. The processName and hostName attributes also show high cardinality, which suggests the need for careful encoding strategies when preparing the data for modeling. An initial examination of the BETH training and test subsets reveals several insights into the nature of system-level activity across cloud-hosted honeypots. These observations have guided the formulation of our LSTM-based anomaly detection model.  

#### 1.1 Event Frequency Distribution. 



```{python}
# ---- FIGURE 1: Most common event names in training data ----
plt.figure(figsize=(10, 5))
top_events = train_df['eventName'].value_counts().nlargest(10)
sns.barplot(x=top_events.values, y=top_events.index, palette='Blues_r')
plt.title("Top 10 Event Names in Training Data")
plt.xlabel("Frequency")
plt.ylabel("Event Name")
plt.tight_layout()
plt.show()
```


As shown in Figure 3, the distribution of system events in the training data is highly skewed. The top three event types close, openat, and security_file_open collectively dominate the logs, each appearing over 150,000 times. In contrast, lower-frequency events such as getdents64 and socket are comparatively rare. This long-tailed distribution suggests that certain system calls represent routine behavior, while others though rare may hold critical information for detecting anomalies. Therefore, proper handling of categorical sparsity and rare event embedding is essential during preprocessing.    

#### 1.2 Process Behavior

```{python}
# ---- FIGURE 2: Most common process names in training data ----
plt.figure(figsize=(10, 5))
top_events = train_df['processName'].value_counts().nlargest(10)
sns.barplot(x=top_events.values, y=top_events.index, palette='viridis')
plt.title("Top 10 process Names in Training Data")
plt.xlabel("Frequency")
plt.ylabel("process_Name")
plt.tight_layout()
plt.show()
```


A similar distribution is observed in processName. A small set of background processes account for a significant fraction of system activity. These are expected in benign operational scenarios. However, malicious processes are often less frequent and unpredictable. This motivates the need for a temporal modeling approach that generalizes from dominant patterns while remaining sensitive to rare deviations.  

#### 1.3 Temporal Skew in Data Collection 

```{python}
# ---- FIGURE 2: Event frequency by hour ----
train_df['hour'] = train_df['timestamp'].dt.hour
plt.figure(figsize=(10, 5))
sns.countplot(data=train_df, x='hour', palette='viridis')
plt.title("System Events Distribution by Hour (Training Data)")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Events")
plt.tight_layout()
plt.show()
```


Figure 4 reveals a pronounced imbalance in the distribution of events across time. The vast majority of events in the training set are concentrated within a single hour of activity, with very limited representation beyond that point. This skew may reflect batch-mode logging or the constrained operational window of honeypot collection. As a result, the model must be designed to generalize temporal patterns from narrow windows of observed behavior, further motivating the use of memory-aware architectures like LSTM.  

#### 1.4 Label Imbalance and Anomaly Prevalence  
```{python}
# ---- FIGURE 3: Count of evil vs. benign events in test set ----
plt.figure(figsize=(6, 4))
sns.countplot(data=test_df, x='evil', palette='coolwarm')
plt.title("Malicious vs. Benign Events (Test Set)")
plt.xlabel("Evil Label")
plt.ylabel("Count")
plt.xticks([0, 1], ['Benign (0)', 'Malicious (1)'])
plt.tight_layout()
plt.show()
```


Contrary to initial expectations, Figure 4 indicates that a substantial portion of the test data is labeled as malicious (evil = 1), with malicious events significantly outnumbering benign ones. This suggests that the dataset may contain attack-heavy test sequences designed for stress-testing anomaly detection models. This observation presents both a challenge and an opportunity: while class imbalance is often a barrier in anomaly detection, the abundance of labeled attacks in the test set allows for richer evaluation of model performance using standard classification metrics such as precision, recall, and F1-score. 

#### 1.4 Correlation

```{python}
def dataset_to_corr_heatmap(dataframe, title, ax):
    corr = dataframe.corr(numeric_only=True)
    sns.heatmap(corr, ax = ax, annot=True, cmap="YlGnBu")
    ax.set_title(f'Correlation Plot for {title}')
```

```{python}
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize = (15,20))
fig.tight_layout(pad=10.0)
datasets = [train_df, test_df, val_df]
dataset_names = ['train', 'test', 'validation']
axs = [ax1, ax2, ax3]

for dataset, name, ax in zip(datasets, dataset_names, axs):
    dataset_to_corr_heatmap(dataset, name, ax)
```

All three of the datasets have a heavy correlation between userId and the associated labels which feature in the dataset (sus and/or evil).
processid and threadid are highly correlated and seem to have similar correlation vaules across all three datasets. This means that they are representing pretty much the same thing and one of them could be dropped.
The correlation plots all look significantly different.

#### 1.4 Boxplot 

```{python}
from scipy import stats
datasets = [train_df, test_df, val_df]

entropy_values = []
for dataset in datasets:
    dataset_entropy_values = []
    for col in dataset.columns:
        if col == 'timestamp':
            pass
        else:
            counts = dataset[col].value_counts()
            col_entropy = stats.entropy(counts)
            dataset_entropy_values.append(col_entropy)

    entropy_values.append(dataset_entropy_values)

plt.boxplot(entropy_values)
plt.title('Boxplot of Entropy Values')
plt.ylabel("entropy values")
plt.xticks([1,2,3],labels=['train', 'test', 'validate'])
plt.show()
```


```{python}
from scipy import stats
datasets = [train_df, test_df, val_df]

variation_values = []
for dataset in datasets:
    dataset_variation_values = []
    for col in dataset.columns:
        if col == 'timestamp':
            pass
        else:
            counts = dataset[col].value_counts()
            col_variation = stats.variation(counts)
            dataset_variation_values.append(col_variation)

    variation_values.append(dataset_variation_values)

plt.boxplot(variation_values)
plt.title('Boxplot of Variation Values')
plt.ylabel("Variation values")
plt.xticks([1,2,3],labels=['train', 'test', 'validate'])
plt.show()

```



#### 1.5 Implications for Modeling  

Taken together, these findings justify our modeling decisions in several ways: The long-tailed event distribution calls for embedding-based representations of categorical variables (e.g., eventName, processName).  

The temporal compression of training data highlights the need for architectures that generalize across short but informative sequences.  

The label distribution in the test set supports a semi-supervised evaluation protocol, where a model trained on benign data is evaluated on its ability to flag deviations (i.e., attacks) as anomalous.  

### 2- Modeling and Results  

##### 2.1 Model Architecture 

The architecture of our anomaly detection system is based on an LSTM autoencoder trained exclusively on benign process sequences. The input to the model is a sliding window of look_back=8 consecutive events, each described by 9 features. The encoder consists of two LSTM layers with 128 and 64 hidden units, respectively, followed by dropout layers to prevent overfitting. The latent representation is repeated and passed to a symmetric decoder consisting of two LSTM layers that reconstruct the input sequence. The model is trained to minimize the Mean Absolute Error (MAE) between the original and reconstructed sequences. 

```{python}
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Assign ground truth: 0=benign (evil=0), 1=malicious (evil=1)
for df in [train_df, val_df, test_df]:
    df['ground_truth'] = df['evil'].apply(lambda x: 1 if x == 1 else 0)

# Select features
features = [
    'processId', 'parentProcessId', 'userId', 'mountNamespace',
    'processName', 'hostName', 'eventId', 'argsNum', 'returnValue'
]

# Label encode categorical columns
for col in ['processName', 'hostName']:
    le = LabelEncoder()
    all_vals = pd.concat([train_df[col], val_df[col], test_df[col]])
    le.fit(all_vals)
    train_df[col] = le.transform(train_df[col])
    val_df[col] = le.transform(val_df[col])
    test_df[col] = le.transform(test_df[col])

# Scale features
scaler = StandardScaler()
train_df[features] = scaler.fit_transform(train_df[features])
val_df[features] = scaler.transform(val_df[features])
test_df[features] = scaler.transform(test_df[features])
```
```{python}
def create_sequences(df, features, look_back=8):
    data_array = df[features].values
    labels = df['ground_truth'].values
    X, y = [], []
    for i in range(len(df) - look_back + 1):
        X.append(data_array[i:i+look_back])
        y.append(labels[i + look_back - 1])
    return np.array(X), np.array(y)


look_back = 8

X_train, _ = create_sequences(train_df, features, look_back)
X_val, _ = create_sequences(val_df, features, look_back)
X_test, y_test = create_sequences(test_df, features, look_back)

print("X_train shape:", X_train.shape)
print("X_val shape:", X_val.shape)
print("X_test shape:", X_test.shape)
print("y_test distribution:", np.bincount(y_test))

```

```{python}
from tensorflow.keras import layers, Model

#LSTM Autoencoder model
def build_lstm_autoencoder(input_shape, latent_dim=64, dropout=0.1, learning_rate=1e-3):
    inputs = layers.Input(shape=input_shape)
    # Encoder
    x = layers.LSTM(128, activation='relu', return_sequences=True)(inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LSTM(latent_dim, activation='relu', return_sequences=False)(x)
    encoded = layers.Dropout(dropout)(x)
    # Repeat vector for decoder (Bottleneck)
    x = layers.RepeatVector(input_shape[0])(encoded)
    # Decoder
    x = layers.LSTM(latent_dim, activation='relu', return_sequences=True)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.LSTM(128, activation='relu', return_sequences=True)(x)
    decoded = layers.TimeDistributed(layers.Dense(input_shape[1]))(x)

    model = Model(inputs, decoded)
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mae')
    return model

input_shape = X_train.shape[1:]
model = build_lstm_autoencoder(input_shape, latent_dim=64, dropout=0.1, learning_rate=1e-3)
model.summary()
```

The final input tensor shape is (batch_size, 8, 9), corresponding to 8 time steps and 9 features. The model has approximately 1.5 million trainable parameters.    


#### 2.2 Training and Validation Performance     

We first evaluate the generalization capability of the model using a 5-fold time-series cross-validation performed on benign data only. For each fold, the model is trained on a different temporal split and evaluated on future sequences to simulate realistic detection scenarios.  

```{python}
#apply cross validaion on 5 fold (val+train)
from sklearn.model_selection import TimeSeriesSplit

full_benign_df = pd.concat([train_df, val_df])
X_benign, _ = create_sequences(full_benign_df, features, look_back=8)

k = 5
tscv = TimeSeriesSplit(n_splits=k)
val_losses = []

for fold, (train_idx, val_idx) in enumerate(tscv.split(X_benign)):
    print(f"\nFold {fold+1}/{k}")
    X_tr, X_val = X_benign[train_idx], X_benign[val_idx]
    model = build_lstm_autoencoder(X_tr.shape[1:])
    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)
    history=model.fit(X_tr, X_tr,
              validation_data=(X_val, X_val),
              epochs=15,
              batch_size=128,
              callbacks=[early_stop],
              verbose=1)
    val_loss = model.evaluate(X_val, X_val, verbose=0)
    print(f"Fold {fold+1} validation loss: {val_loss:.6f}")
    val_losses.append(val_loss)

print(f"\nMean validation loss over {k} folds: {np.mean(val_losses):.6f} (+/- {np.std(val_losses):.6f})")
model.save('/content/drive/MyDrive/Dataset_capstone/model.h5')


```
```{python}
model.save("my_model.keras")
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


The average validation loss across folds was: 0.142291 (+/-0.128335). After cross-validation, the model was retrained on the full benign training set. The final training and validation losses indicate stable convergence without overfitting as we can see on the figure above


#### 2.3 Anomaly Detection Threshold
The trained model was then used to compute reconstruction errors over the test set. The distribution of reconstruction errors clearly shows a distinguishable separation between normal and anomalous sequences 

```{python}
# Predict reconstructions
X_test_pred = model.predict(X_test)
test_mae = np.mean(np.abs(X_test_pred - X_test), axis=(1,2))  # Mean absolute error per sequence

plt.figure(figsize=(10,4))
plt.hist(test_mae, bins=100, alpha=0.6)
plt.title("Distribution of Reconstruction Error (Test Set)")
plt.xlabel("MAE")
plt.ylabel("Count")
plt.show()
```

To convert reconstruction error into binary predictions, we used the precision-recall curve to find the optimal threshold that maximizes the F1-score: 0.9549.

```{python}
precision, recall, thresholds = precision_recall_curve(y_test, test_mae)
f1 = 2 * (precision * recall) / (precision + recall + 1e-10)
best_threshold = thresholds[np.argmax(f1)]
print(f"Best threshold by max F1: {best_threshold:.4f}")
plt.plot(thresholds, precision[:-1], label='Precision')
plt.plot(thresholds, recall[:-1], label='Recall')
plt.plot(thresholds, f1[:-1], label='F1')
plt.axvline(best_threshold, color='r', linestyle='--', label='Best threshold')
plt.xlabel('Threshold')
plt.legend()
plt.title("Precision/Recall/F1 vs Threshold")
plt.show()

```


#### 2.4 Classification Metrics
Once the best threshold was determined, we applied it to the test set. The model successfully identified anomalous sequences with high accuracy of 99%. The confusion matrix below shows that most benign and malicious events are correctly classified.  
```{python}
# Predict anomalies
y_pred = (test_mae > best_threshold).astype(int)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# Metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1s = f1_score(y_test, y_pred)
fpr = cm[0,1] / (cm[0,0] + cm[0,1])
fnr = cm[1,0] / (cm[1,0] + cm[1,1])
roc_auc = roc_auc_score(y_test, test_mae)
```

```{python}
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',
            xticklabels=['Benign (Pred)', 'Malicious (Pred)'],
            yticklabels=['Benign (True)', 'Malicious (True)'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()
```


- False Positive Rate: 0.040
- False Negative Rate: 0.005

The confusion matrix highlights the model’s strength in correctly identifying malicious events (true positives), while maintaining a relatively low rate of false positives. Misclassifications were analyzed and often linked to sequences with novel process or host identifiers not seen during training further confirming the model’s ability to flag unfamiliar temporal patterns. 

The high recall suggests the model captures almost all malicious behavior, which is critical for a cybersecurity context. The small false positive rate (3.4%) indicates that few benign sequences are misclassified, making this model practical for real-time deployment where false alarms are costly. The ROC curve and AUC value confirm the model's excellent discriminatory power between normal and malicious activity.

```{python}
# ROC Curve
fpr_curve, tpr_curve, _ = roc_curve(y_test, test_mae)
plt.plot(fpr_curve, tpr_curve, label=f'ROC AUC={roc_auc:.2f}')
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("ROC Curve")
plt.legend()
plt.show()
```
 

#### 2.5 Overall Evaluation and Future Considerations
The LSTM autoencoder exhibited strong performance in terms of: Learning a stable representation of normal sequences. Flagging novel and anomalous patterns through unsupervised learning. Achieving a high F1-score despite class imbalance.
Aligning with state-of-the-art performance in similar studies [@highnam2021beth; @kim2016long].
For future deployment, periodic retraining is recommended to adapt to system evolution. Additionally, incorporating attention mechanisms or hybrid ensembles may further enhance sensitivity to subtle anomalies.

## IV-Conclusion

This project has demonstrated the substantial potential of LSTM autoencoders for detecting anomalous cybersecurity events in real-world, process-level system logs. By leveraging the BETH dataset—one of the most realistic and contemporary benchmarks for intrusion detection—we were able to rigorously assess the effectiveness of deep sequence modeling for practical cyber defense.
Our approach, grounded in unsupervised learning, involved training the LSTM autoencoder exclusively on benign sequences to learn the complex temporal dependencies that characterize normal system behavior. The model’s ability to generalize from limited windows of typical activity allowed it to sensitively identify even subtle deviations linked to novel or sophisticated attacks. Through careful feature engineering, one-hot encoding, and sequence modeling, we prepared heterogeneous system logs for robust anomaly detection.

The evaluation of our model was comprehensive and multidimensional. Using metrics such as precision, recall, F1-score, ROC-AUC, and confusion matrices, we showed that the LSTM autoencoder achieved high accuracy (99%) and a strong ability to discriminate between benign and malicious activity—even in the face of class imbalance and temporal data skew. The model’s low false positive and false negative rates affirm its practicality for real-time deployment, where both missed attacks and false alarms carry significant costs.

A key strength of this work lies in its adaptability. Because the method is unsupervised and does not rely on prior knowledge of attack signatures, it remains robust against previously unseen or zero-day threats. This property, combined with rigorous threshold tuning and cross-validation, positions the approach as a promising candidate for deployment in dynamic, evolving cybersecurity environments.

**Limitations and Future Work:** While the results are compelling, some limitations remain. The dataset’s temporal compression and event imbalance, as well as potential overfitting to benign process patterns, suggest the need for ongoing retraining as system behaviors evolve. Future enhancements could include integrating attention mechanisms, hybrid ensemble methods, or continual learning strategies to further improve detection sensitivity and adaptability.

In summary, this project bridges advanced deep learning research with operational cybersecurity needs. By demonstrating that LSTM autoencoders can reliably flag anomalies in complex system logs without extensive labeled attack data, we provide a practical, scalable, and interpretable framework for next-generation intrusion detection. Our findings underscore the critical role of sequence-aware models in defending modern digital infrastructures against a rapidly evolving threat landscapes
## References
