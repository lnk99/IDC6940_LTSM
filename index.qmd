---
title: "Detecting Anomalous Cybersecurity Events Using LSTM Autoencoders"
subtitle: "A Case Study on the BETH Dataset"
author: "Lionel & Killian (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: inline
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

As modern digital infrastructures become increasingly interconnected and complex, attackers are relying more on process-level intrusions that carefully blend into legitimate activity. Such sophisticated threats often unfolding slowly over time or interleaving malicious commands with normal operations can evade static, signature-based defenses. Traditional systems, which are designed around pre-defined rules or pattern matching, fail to account for subtle changes in temporal context: an action that appears normal in isolation may signal an attack when it occurs in an unusual sequence. Detecting these threats requires models capable of understanding how events unfold over time.

This challenge has spurred research toward dynamic, data-driven sequence modeling methods. Among these, Long Short-Term Memory (LSTM) networks have emerged as particularly effective. Introduced by Hochreiter and Schmidhuber [@hochreiter1997long], LSTM networks use gated memory cells that retain information across long sequences, effectively overcoming the vanishing gradient problem in traditional recurrent neural networks. This architecture allows LSTMs to capture dependencies spanning dozens or even hundreds of events, making them well-suited for modeling the temporal structure of cybersecurity logs, where the sequence of system calls and process operations is often critical.

The versatility of LSTM networks has been demonstrated across diverse domains. In speech recognition, Graves, Mohamed, and Hinton [@graves2013speech] showed that deep LSTM architectures can learn the temporal structure of audio signals, significantly reducing error rates on benchmark tasks. In natural language processing, Sundermeyer et al. [@sundermeyer2012lstm] used LSTMs for language modeling, demonstrating that the model’s ability to retain long-range context leads to more accurate text generation compared to traditional n-gram methods. In the clinical domain, Lipton et al. [@lipton2016learning] applied LSTM models to patient vitals time series, revealing how memory-enabled models can detect subtle health anomalies as they develop. Across these applications, a core insight emerges: when the data is fundamentally sequential, LSTMs excel at modeling it.

This modeling power has translated into cybersecurity research as well. Kim et al. [@kim2016long] applied LSTM-based classifiers to intrusion detection, demonstrating that LSTMs can recognize abnormal sequences of system and network behavior with lower false-positive rates than static feature approaches. Malhotra et al. [@malhotra2016lstm] advanced this work by introducing an LSTM encoder decoder for anomaly detection. Instead of directly classifying events, the model reconstructs input sequences, identifying anomalies by elevated reconstruction errors an approach especially well suited to unlabeled or novel attacks, as it learns solely from normal behavior. Yin et al. [@yin2017deep] extended this concept using recurrent neural networks over raw flow data, achieving improved generalization in network intrusion detection. Building further, Cinque et al. [@cinque2022micro2vec] introduced Micro2vec, which embeds log message patterns as numeric vectors and leverages LSTM networks to detect anomalies in microservice environments, showing how sequence models can scale to complex, real-world systems.

Yet, progress in these models has often been constrained by outdated benchmarks. Seminal datasets such as KDD Cup 1999 [@hettich1999uci], NSL-KDD [@tavallaee2009detailed], and ISCX 2012 [@shiravi2012developing] provided early test beds for intrusion detection research, but are now hampered by synthetic traffic, unrealistic features, and obsolete system contexts limiting their generalizability. A model trained on these datasets may detect known patterns, but often fails in real-world scenarios where attackers use new commands, timing patterns, or process chains.

In response, Highnam et al. [@highnam2021beth] introduced the BETH dataset, marking a significant advance in realism and operational relevance. Instrumented with eBPF across live honeypots, BETH captures over eight million process-level events including system calls, arguments, return values, and timestamps. Importantly, each data instance cleanly separates benign system behavior from adversarial activity, providing labeled, temporally coherent sequences. This makes BETH an ideal platform for training sequence-based anomaly detection models in a semi-supervised setting.

Our approach builds directly on this foundation. We deploy an LSTM autoencoder trained exclusively on benign process sequences from BETH. The encoder compresses temporal sequences into fixed-length latent representations, while the decoder attempts to reconstruct them. When the model encounters novel malicious behavior, its reconstruction fails, producing high reconstruction error, this serves as our anomaly signal. This method aligns with recent advances in unsupervised anomaly detection [@gokstorp2024anomaly; @greff2017lstm], removing the need for prior knowledge of attack types and allowing rapid adaptation to emerging threats.

This work makes three key contributions. First, it demonstrates the effectiveness of LSTM-based sequence modeling for detecting anomalies in real-world, process-level logs. Second, it presents a rigorous evaluation on the BETH dataset, using classification metrics and reconstruction thresholds to quantify detection capabilities. Third, it bridges the gap between advanced deep learning techniques and practical cybersecurity needs, delivering an interpretable, flexible, and deployable detection framework grounded in realistic system behavior.


## Methods
This section presents the methodological framework for detecting anomalies in cybersecurity event logs using Long Short-Term Memory (LSTM) autoencoders. The approach is organized into six interconnected components and together, these steps form a comprehensive and scalable pipeline for identifying irregularities in large-scale log data

### 1. Long Short-Term Memory (LSTM) Networks

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) developed by Hochreiter and Schmidhuber (1997) [@hochreiter1997long] specifically to overcome the vanishing gradient problem found in traditional RNNs. In cybersecurity applications, the ability to model long-range temporal dependencies is crucial, as malicious activity often unfolds over extended sequences of system calls, network packets, or access attempts.

An LSTM cell maintains two key internal states at each time step: the cell state $C_t$ and the hidden state $h_t$. The cell state serves as a memory that runs across the sequence, allowing information to persist over time. Three gating mechanisms regulate this process:

- **Forget Gate**: Determines which parts of the previous memory to discard.  
  $$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$$
- **Input Gate and Candidate Update**: Controls how much new information is added to the memory.  
  $$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i), \;\; \tilde{C}_t = \tanh(W_c [h_{t-1}, x_t] + b_c)$$
- **Cell State Update**:  
  $$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$
- **Output Gate and Hidden State**: Controls what the cell outputs.  
  $$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o), \;\; h_t = o_t * \tanh(C_t)$$
![Figure 1: A module of LSTM network.](lstm_cell.png)
      $$\text {Figure 1: A module of LSTM network}.$$  
LSTM’s memory mechanisms make it especially effective for modeling system behavior over time, where a sequence of seemingly benign events may, when considered together, signal an attack. This makes LSTM a preferred choice for modeling logs in intrusion detection systems [@lipton2016learning; @greff2017lstm; @graves2013speech].

### 2. LSTM Autoencoder for Anomaly Detection

An LSTM autoencoder combines an encoder–decoder architecture with LSTM layers to learn latent representations of temporal data in an unsupervised manner. The encoder compresses input sequences into a fixed-length latent vector, while the decoder attempts to reconstruct the sequence from this vector. If the model is trained exclusively on normal data, it learns to reconstruct familiar sequences accurately. Anomalous sequences, which deviate from learned patterns, yield high reconstruction error a signal of abnormal behavior.

Formally, for an input sequence $x = (x_1, ..., x_T)$, the autoencoder is trained to minimize the reconstruction loss:

$$
L = \frac{1}{T} \sum_{t=1}^T \| x_t - \hat{x}_t \|^2
$$

where $\hat{x}_t$ is the output generated by the decoder at each time step.

During inference, this reconstruction error serves as an anomaly score. A sequence with error above a threshold $\theta$ is flagged as anomalous. This technique, called reconstruction-based anomaly detection, is effective when anomalous samples are rare or unknown, which is often the case in cybersecurity contexts.

Past research has demonstrated the effectiveness of this approach in domains such as multi-sensor fault detection [@malhotra2016lstm], system log anomaly detection [@gokstorp2024anomaly], and network intrusion detection [@kim2016long; @cinque2022micro2vec]. Its unsupervised nature and ability to model complex time-dependent features make it a natural fit for the challenges of modern intrusion detection systems.

### 3. Anomaly Detection Using LSTM Autoencoders

Once trained on normal data, an LSTM autoencoder becomes a powerful tool for identifying anomalous behavior in sequential inputs such as system logs or network traces. The core idea is that the model becomes highly proficient at reconstructing sequences that follow the familiar temporal patterns it learned during training. However, when presented with sequences that deviate from these learned patterns such as those containing rare, unexpected, or malicious events, the model fails to reconstruct them accurately, producing a noticeably higher reconstruction error.

This reconstruction error serves as a quantifiable measure of anomaly. Specifically, the anomaly score for a given input sequence is computed as the **mean squared error (MSE)** between each input vector  $x_t$ and its reconstructed counterpart $x_t$:


$$\text{Reconstruction Error} = \frac{1}{T} \sum_{t=1}^{T} \|x_t - \hat{x}_t\|^2$$

Here,  $T$ is the total number of time steps in the sequence. A higher score indicates greater divergence from normal behavior, and thus, a higher likelihood that the sequence is anomalous.

To operationalize this detection process, a **threshold** is established. This threshold is usually selected by analyzing the distribution of reconstruction errors in the training or validation dataset, often by selecting a statistical percentile or optimizing detection metrics such as the F1-score. Any sequence with a reconstruction error exceeding the threshold is classified as **anomalous**, while sequences with lower errors are assumed to represent **normal activity**.

This approach offers several key benefits in the context of cybersecurity anomaly detection:

- **Unsupervised learning**: There is no need for labeled attack data during training, which is ideal since real-world attack examples are rare, imbalanced, or unknown at the time of deployment.
- **Temporal awareness**: LSTM cells allow the model to consider context across long event sequences—making it effective in identifying subtle but meaningful deviations in system behavior over time.
- **Detection of unknown attacks**: Since the model is not trained on known attack signatures, it generalizes better to novel or zero-day threats, simply by recognizing that "this sequence doesn't look like normal behavior."

The method has been successfully used in a variety of security-critical environments. For instance, Malhotra et al. [@malhotra2016lstm] used it to detect failures in multi-sensor environments, while Gökstorp et al. [@gokstorp2024anomaly] applied it to analyze security logs. These studies show that LSTM autoencoders can reliably distinguish benign from anomalous sequences, even in high-noise or high-volume contexts.

In this project, we leverage the LSTM autoencoder’s ability to detect such behavioral anomalies in the BETH dataset. By training solely on benign log sequences and evaluating reconstruction performance on new data, our model provides a flexible, adaptive, and scalable mechanism for real-time anomaly detection.

### 4. Justification for Approach and Dataset

Cybersecurity anomaly detection requires models that are flexible, context-aware, and capable of adapting to new or unseen attack patterns. Traditional rule-based or signature-based detection systems are limited by their reliance on predefined patterns, making them ineffective against zero-day attacks or novel behavior.

Unsupervised learning, particularly autoencoder-based methods, allows us to model what “normal” looks like and detect deviations from it, without requiring labeled attacks. This approach is grounded in the notion that abnormal behavior is, by definition, rare and structurally distinct from routine system activity [@yin2017deep].

The BETH dataset [@highnam2021beth] was selected to evaluate our approach. It includes over 8 million system log events collected from 23 honeypots designed to mimic real-world server environments. Each honeypot was exposed to the internet and captured one targeted attack (at most), alongside a large volume of benign activity. Events are labeled using two binary fields: Sus for suspicious activity and Evil for confirmed malicious events.

**The BETH dataset offers several key advantages:**  
- **Realism:** Data is collected in production-like environments with realistic attack vectors.  
- **Anomaly-focused labeling:** The Sus and Evil labels allow fine-grained evaluation of anomaly detection systems.  
- **Accessibility:** The dataset is publicly available, enabling reproducibility and peer comparison.  

Compared to older datasets like KDD’99 [@tavallaee2009detailed; @hettich1999uci] or NSL-KDD, BETH reflects modern system-level behavior and attack techniques, providing a more accurate benchmark for current anomaly detection research.

### 5. Data Preprocessing and Sequence Modeling

To prepare the BETH dataset for LSTM modeling, we applied the following preprocessing steps:

- **Feature extraction:** Key fields such as Timestamp, EventID, SyscallType, ProcessId, ReturnValue, and ArgsNum were selected based on relevance to behavioral analysis.
- **Encoding:** Categorical fields were one-hot encoded to retain discrete information. Numerical fields were normalized to the [0, 1] range using min-max scaling.
- **Sequence generation:** A sliding window of fixed size (e.g., 25–50 events) was used to convert raw logs into overlapping sequences. Each sequence was reshaped into a 3D tensor of shape (Batch × Time × Features), preserving temporal order and structural coherence.

**Labeling for sequences was handled as follows:**
- Sequences with only benign events were labeled as normal.
- Sequences containing at least one Sus or Evil event were labeled as anomalous.

This approach mirrors real-world monitoring scenarios, where logs are processed in fixed-length windows to detect behavioral anomalies over time.

### 6. Model Training, Thresholding, and Evaluation

#### Training and Architecture

The LSTM autoencoder is trained solely on normal sequences. The architecture includes two stacked LSTM layers in the encoder and decoder, with a fully connected layer at the bottleneck. The decoder mirrors the encoder in structure, and the model is trained to minimize reconstruction loss using the Adam optimizer.

To improve generalization:
- Early stopping is used based on validation loss.
- A dropout layer is added to mitigate overfitting.
- 5-fold cross-validation is used to assess the stability of the model across different subsets of the benign data.

Hyperparameters (e.g., learning rate, batch size, window size, latent vector size) were selected based on preliminary tuning and aligned with best practices from related work [@nguyen2021forecasting; @malhotra2016lstm].

#### Threshold Selection

After training, we determine an anomaly threshold $\theta$ by analyzing reconstruction errors on a labeled validation set. Multiple strategies are considered:

- Using a fixed percentile of training error distribution (e.g., 95th percentile)
- Optimizing the F1-score on validation anomalies
- Minimizing false positive and false negative rates jointly

The selected threshold is applied to test sequences during evaluation.

#### Evaluation Metrics

The model’s performance is evaluated using the following metrics:

- **Precision:** The proportion of predicted anomalies that are true anomalies.
- **Recall:** The proportion of actual anomalies that are correctly identified.
- **F1-score:** The harmonic mean of precision and recall.
- **ROC-AUC:** Measures the trade-off between true positive and false positive rates.
- **Confusion Matrix:** Provides a granular view of model classification results.
- **Error distribution plots:** Visualize separation between normal and anomalous samples.

These metrics provide a robust, multidimensional assessment of the model’s effectiveness in identifying anomalous behavior.

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
