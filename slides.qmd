---
title: "Detecting Anomalous Cybersecurity Events Using LSTM Networks"
subtitle: "A Case Study on the BETH Dataset"
author: "Lionel & Killian (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  revealjs
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

- Modern infrastructures face **sophisticated, process-level intrusions** that mimic benign behavior.
- **Static rule-based detection fails** to capture temporal dependencies.
- We investigate **LSTM-based anomaly detection** for process logs.
- **Key idea**: model normal system activity and detect deviations as anomalies.

---

## LSTM Motivation {.smaller}

- **LSTM networks** (Hochreiter et al. 1997):
  - Solve vanishing gradient problem with gated memory cells.
  - Capable of learning **long-range temporal dependencies**.

- Applications:
  - **Speech recognition** (Graves et al. 2013).
  - **Language modeling** (Sundermeyer et al. 2012).
  - **Clinical anomaly detection** (Lipton et al. 2016).

- Why relevant ?
  - **Cybersecurity logs are sequential** — order and timing matter.

---

## Cybersecurity Context 

- **Kim et al. 2016**: LSTM classifiers for intrusion detection outperform static baselines.


- **Malhotra et al. 2016**: LSTM encoder-decoder reconstructs normal sequences, anomalies detected by high reconstruction error.


- **Yin et al. 2017**: RNNs generalize better on raw network flows.


- **Cinque et al. 2022**: Micro2vec + LSTM captures log anomalies in microservices.

---

## Data Challenge 

- Old datasets (KDD’99, NSL-KDD, ISCX 2012) are **synthetic and outdated**.


- **BETH dataset (Highnam et al. 2021)**:
  - 8M+ process-level events (eBPF-instrumented honeypots).
  - Rich features: timestamps, syscalls, labels (benign vs. malicious).
  - Realistic **temporal and adversarial patterns** for modern systems.

---

## Our Approach 

- **Semi-supervised LSTM Autoencoder**:
  - Train only on benign sequences (normality modeling).
  - Use **reconstruction error** to flag anomalies.

- **Goal**: detect process-level attacks in test logs.

---

## Methods

(Will be added later)

---

## Data Exploration and Visualization

- **Dataset size**: 8,004,918 system events.

- **Training subset**: ~763,145 benign events (60/20/20 split).

- Features:  
  - Process/thread identifiers, processName, eventName.
  - Syscall arguments, return values.
  - Binary labels: `sus` (suspicious) and `evil` (malicious).

---

## Event Distributions {.smaller}


**Figure 1:** Top 10 most common event names in the training data.



**Figure 2:** Distribution of events over time by hour.


---

**Figure 3:** Frequency of anomalous vs. benign events in the test set.

## Modeling and Results {.smaller} 

### A. Model Architecture

*LSTM Autoencoder:*

Encoder: LSTM(128) → LSTM(64) + Dropout.

Decoder: LSTM(64) → LSTM(128) reconstructing input.

Input shape: (batch_size, 8, 9).

**Figure 4:** Model architecture


## B. Training and Validation Performance {.smaller}

- 5-fold cross-validation:

  - Avg. validation loss: **0.1516 ± 0.1443.**

- Stable convergence.

**Figure 5:** Training & Validation Performance



## C. Anomaly Detection Threshold {.smaller}

Distribution of reconstruction error separates normal vs. anomalous.

**Figure 6:** Reconstruction Error Distribution


## D. Classification Metrics {.smaller}

Precision: 99.3%, Recall: 99.5%, F1-score: 99.4%.

**Figure 7:** Precision/Recall Curve

## Confusion Matrix {.smaller}

**Figure 8:** Confusion Matrix

## ROC Curve {.smaller}

ROC AUC = 99.5%.

**Figure 9:** ROC Curve

## Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References

